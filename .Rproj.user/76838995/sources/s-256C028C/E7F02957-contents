#takes seed
set.seed((40))
#read the file and remove the ? rows
cen.data = read.csv("census-adult.csv",na.strings = " ?")
cen.data = na.omit(cen.data)

#take a sample and split the data into training and test 60:40
indices = sample(1:nrow(cen.data), 2000)
train = indices[801:2000]
test = indices[1:800]
test_set = cen.data[test,]
train_set = cen.data[train,]
 
#calculate the entropy
'
entropy = function(column)
{
  values = table(column)
  probablitites = values/length(column)
  
   
  
}
'
entropy <- function(val) {
  freq <- table(val)/length(val)
  # vectorize
  vec <- as.data.frame(freq)[,2]
  #drop 0 to avoid NaN resulting from log2
  vec<-vec[vec>0]
  #compute entropy
  -sum(vec * log2(vec))
}


#calculate the information gain for each of the columns (use a user-defined function)
infoGain = function(node, colname, label.column)
{
  
  0
}


print(entropy(cen.data$income))







'
print(cen.data)
label = data.frame(cen.data$income)
print(class(cen.data))
print(summary(label))
newdata = subset(cen.data, select  = -c(income))
#print((newdata))
print(names(cen.data))
'

'
formula_list = (paste(names(newdata), sep = '+', collapse = '+'))
formula_list = (paste(c("income",formula_list), collapse= '~'))
print(formula_list)
formula_exp = formula(formula_list)
print(c(class(cen.data), class(formula_exp)))
#print(kyphosis)
tree.income = rpart(formula = formula_exp , data = cen.data, method = "class", parms = list(split = "information"))
plot(tree.income, uniform=TRUE, 
     main="Classification Tree for Income")
text(tree.income, use.n=TRUE, all=TRUE, cex=.8)
post(tree.income, file = "tree.ps", 
     title = "Classification Tree for Income")
print(tree.income)
    
     '